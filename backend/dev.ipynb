{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scenario\n",
    "from copy import deepcopy\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from constants import DATA_ENTITIES_DIR, DATA_HAZARDS_DIR, DATA_TEMP_DIR, REQUIREMENTS_DIR\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from climada.engine import CostBenefit, Impact, ImpactCalc\n",
    "from climada.engine.cost_benefit import risk_aai_agg, risk_rp_100, risk_rp_250\n",
    "from climada.entity import DiscRates, Entity, Exposures, LitPop\n",
    "from climada.entity.impact_funcs import ImpactFunc, ImpactFuncSet, ImpfTropCyclone\n",
    "from climada.entity.impact_funcs.storm_europe import ImpfStormEurope\n",
    "from climada.entity.impact_funcs.trop_cyclone import ImpfSetTropCyclone\n",
    "from climada.entity.measures import Measure, MeasureSet\n",
    "from climada.hazard import Hazard\n",
    "from climada.util.api_client import Client\n",
    "\n",
    "from costben.costben_handler import CostBenefitHandler\n",
    "from entity.entity_handler import EntityHandler\n",
    "from exposure.exposure_handler import ExposureHandler\n",
    "from hazard.hazard_handler import HazardHandler\n",
    "from impact.impact_handler import ImpactHandler\n",
    "\n",
    "from base_handler import BaseHandler\n",
    "from logger_config import LoggerConfig\n",
    "\n",
    "logger = LoggerConfig(logger_types=[\"file\"])\n",
    "\n",
    "base_handler = BaseHandler()\n",
    "costben_handler = CostBenefitHandler()\n",
    "entity_handler = EntityHandler()\n",
    "exposure_handler = ExposureHandler()\n",
    "hazard_handler = HazardHandler()\n",
    "impact_handler = ImpactHandler()\n",
    "\n",
    "\n",
    "# Available Exposure data types in CLIMADA API for Egypt/Thailand: ['litpop']\n",
    "# Available Hazard data types in CLIMADA API for Egypt/Thailand: ['river_flood', 'wildfire', 'earthquake', flood, 'tropical_cyclone']\n",
    "# Available climate scenarios for hazard type river_flood/tropical_c in country Egypt/Thailand: ['rcp26', 'historical', 'rcp60', 'rcp85']\n",
    "# Available time horizons for hazard type river_flood in country Egypt: ['2030_2050', '1980_2000', '2070_2090', '2010_2030', '2050_2070']\n",
    "\n",
    "country_name = \"Egypt\"\n",
    "exposure_type = \"litpop\"  # Available exposure types for Egypt/Thailand: ['litpop']\n",
    "hazard_type = \"river_flood\"  # Available hazard types for Egypt/Thailand: ['river_flood', 'wildfire', 'earthquake', 'flood', 'tropical_cyclone']\n",
    "scenario = \"rcp26\"  # Available scenarios for Egypt/Thailand: ['rcp26', 'historical', 'rcp60', 'rcp85']\n",
    "time_horizon = \"2030_2050\"  # Available time horizons for Egypt/Thailand: ['2030_2050', '1980_2000', '2070_2090', '2010_2030', '2050_2070']\n",
    "annual_growth = 1.01\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# growth_rates = {\n",
    "#     \"Egypt\": {\n",
    "#         \"crops\": 0.04,\n",
    "#         \"livestock\": 0.04,\n",
    "#         \"power_plants\": 0.04,\n",
    "#         \"hotels\": 0.04,\n",
    "#         \"hospitalised_people\": 0.0129,\n",
    "#         \"students\": 0.0129,\n",
    "#         \"diarrhea_patients\": 0.0129,\n",
    "#         \"roads\": 0.0129,\n",
    "#     },\n",
    "#     \"Thailand\": {\n",
    "#         \"tree_crops\": 0.0294,\n",
    "#         \"grass_crops\": 0.0294,\n",
    "#         \"wet_markets\": 0.0294,\n",
    "#         \"grass_crops_farmers\": -0.0022,\n",
    "#         \"tree_crops_farmers\": -0.0022,\n",
    "#         \"buddhist_monks\": -0.0022,\n",
    "#         \"diarrhea_patients\": -0.0022,\n",
    "#         \"students\": -0.0022,\n",
    "#         \"roads\": -0.0022,\n",
    "#     },\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data files\n",
    "import h5py\n",
    "import rasterio\n",
    "\n",
    "def read_mat():\n",
    "    with h5py.File(DATA_HAZARDS_DIR / \"hazard_HW_EGY_rcp45.mat\", 'r') as f:\n",
    "        # Print all items in the root\n",
    "        print(\"Items in the root:\", list(f.keys()))\n",
    "\n",
    "        # Access the 'hazard' group/dataset\n",
    "        hazard = f['hazard']\n",
    "        print(\"Items in 'hazard':\", list(hazard.keys()))\n",
    "\n",
    "        # If 'hazard' contains further groups or datasets, access them\n",
    "        for item in hazard:\n",
    "            print(f\"Exploring {item}:\")\n",
    "            data = hazard[item]\n",
    "            \n",
    "            # Check if this is a dataset or a group\n",
    "            if isinstance(data, h5py.Dataset):\n",
    "                print(f\"Dataset {item} found with shape {data.shape} and data type {data.dtype}\")\n",
    "            else:\n",
    "                print(f\"Group {item} contains: {list(data.keys())}\")\n",
    "\n",
    "            # If the item is stored by reference (common in MATLAB structures)\n",
    "            if data.dtype == 'O':  # Object references\n",
    "                # This will go through each reference and try to resolve it\n",
    "                for ref in data:\n",
    "                    referenced_object = f[data[ref][0]]  # Access by reference\n",
    "                    print(f\"Referenced object for {ref}: {referenced_object.shape}\")\n",
    "\n",
    "                    # Optionally, you can load the data into an array or similar\n",
    "                    # print(np.array(referenced_object))\n",
    "\n",
    "def read_tif():\n",
    "    with rasterio.open(DATA_HAZARDS_DIR / \"hazard_HW_EGY_rcp45.tif\") as src:\n",
    "        print(f'Number of bands: {src.count}')\n",
    "        # Loop through each band\n",
    "        # for i in range(1, src.count + 1):\n",
    "        #     band = src.read(i)\n",
    "        #     print(band)\n",
    "        #     meta = src.tags(i)\n",
    "        #     print(f'  Metadata for Band {i}: {meta}')\n",
    "\n",
    "\n",
    "\n",
    "def explore_h5_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads and explores an HDF5 (.h5) file, printing its structure and basic data info.\n",
    "\n",
    "    :param file_path: The path to the HDF5 file.\n",
    "    :type file_path: str or Path\n",
    "    \"\"\"\n",
    "    def explore_group(group, level=0):\n",
    "        \"\"\"\n",
    "        Recursively explores a group in the HDF5 file, printing dataset information.\n",
    "\n",
    "        :param group: The HDF5 group to explore.\n",
    "        :type group: h5py.Group\n",
    "        :param level: The current depth level in the group hierarchy (for indentation).\n",
    "        :type level: int\n",
    "        \"\"\"\n",
    "        indent = \"  \" * level  # To visualize hierarchy\n",
    "\n",
    "        for key in group.keys():\n",
    "            item = group[key]\n",
    "            if isinstance(item, h5py.Group):\n",
    "                print(f\"{indent}Group: {key}\")\n",
    "                # Recursively explore this group\n",
    "                explore_group(item, level + 1)\n",
    "            elif isinstance(item, h5py.Dataset):\n",
    "                print(f\"{indent}Dataset: {key}\")\n",
    "                print(f\"{indent}  Shape: {item.shape}\")\n",
    "                print(f\"{indent}  Data type: {item.dtype}\")\n",
    "                # Print attributes if available\n",
    "                if item.attrs:\n",
    "                    print(f\"{indent}  Attributes: {dict(item.attrs)}\")\n",
    "            else:\n",
    "                print(f\"{indent}Other: {key}\")\n",
    "\n",
    "    # Open the HDF5 file\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        print(f\"Exploring HDF5 file: {file_path}\")\n",
    "        # Explore the root group\n",
    "        explore_group(f)\n",
    "\n",
    "# Example usage\n",
    "explore_h5_file(DATA_HAZARDS_DIR / \"Thai_DR_RCP_45.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Measure      Cost (USD )    Benefit (USD )    Benefit/Cost\n",
      "---------  -------------  ----------------  --------------\n",
      "SWP          6.61653e+06                 0               0\n",
      "CSA          1.28705e+07                 0               0\n",
      "WSS          2.142e+07                   0               0\n",
      "WDF          1.05e+08                    0               0\n",
      "\n",
      "--------------------  -  ------\n",
      "Total climate risk:   0  (USD )\n",
      "Average annual risk:  0  (USD )\n",
      "Residual risk:        0  (USD )\n",
      "--------------------  -  ------\n",
      "Net Present Values\n"
     ]
    }
   ],
   "source": [
    "# Example App\n",
    "entity_present = entity_handler.get_entity_from_xlsx(\n",
    "    DATA_ENTITIES_DIR / \"entity_TODAY_THA_D_grass_crops.xlsx\" # type: ignore\n",
    ")\n",
    "entity_present.check()\n",
    "entity_present.exposures.ref_year = 2024\n",
    "\n",
    "entity_future = entity_handler.get_future_entity(entity_present, 2050, 0.0294)\n",
    "entity_future.check()\n",
    "\n",
    "hazard_present = hazard_handler.get_hazard(\n",
    "    hazard_type=\"drought\", filepath=DATA_HAZARDS_DIR / \"hazard_D_THA_historical.h5\"\n",
    ")\n",
    "hazard_present.haz_type = \"D\"\n",
    "hazard_present.check()\n",
    "\n",
    "hazard_future = hazard_handler.get_hazard(\n",
    "    hazard_type=\"drought\", filepath=DATA_HAZARDS_DIR / \"hazard_D_THA_rcp45.h5\"\n",
    ")\n",
    "hazard_future.haz_type = \"D\"\n",
    "hazard_future.check()\n",
    "\n",
    "hazard_present.units = hazard_handler.get_hazard_intensity_units_from_entity(entity_present)\n",
    "hazard_future.units = hazard_handler.get_hazard_intensity_units_from_entity(entity_future)\n",
    "\n",
    "hazard_present.centroids.set_geometry_points()\n",
    "hazard_future.centroids.set_geometry_points()\n",
    "\n",
    "hazard_present.intensity_thres = hazard_handler.get_hazard_intensity_thres(\"D\")\n",
    "hazard_future.intensity_thres = hazard_handler.get_hazard_intensity_thres(\"D\")\n",
    "\n",
    "# Calculate impact\n",
    "impact_present = ImpactCalc(\n",
    "    entity_present.exposures, entity_present.impact_funcs, hazard_present\n",
    ").impact(save_mat=True)\n",
    "impact_future = ImpactCalc(\n",
    "    entity_future.exposures, entity_future.impact_funcs, hazard_future\n",
    ").impact(save_mat=True)\n",
    "\n",
    "\n",
    "costben = costben_handler.calculate_cost_benefit(\n",
    "    hazard_present, entity_present, hazard_future, entity_future, 2050\n",
    ")\n",
    "# costben_handler.plot_cost_benefit(costben)\n",
    "# costben_handler.plot_waterfall(\n",
    "#     costben, hazard_present, entity_present, hazard_future, entity_future\n",
    "# )\n",
    "# ax = costben.plot_waterfall(hazard_present, entity_present, hazard_future, entity_future)\n",
    "# costben.plot_cost_benefit()\n",
    "\n",
    "# exposure_handler.generate_exposure_geojson(entity_future.exposures, \"Thailand\")\n",
    "# hazard_handler.generate_hazard_geojson(hazard_future, \"Thailand\")\n",
    "# impact_handler.generate_impact_geojson(impact_future, \"Thailand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>value</th>\n",
       "      <th>value_unit</th>\n",
       "      <th>deductible</th>\n",
       "      <th>cover</th>\n",
       "      <th>impf_</th>\n",
       "      <th>region_id</th>\n",
       "      <th>centr_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202</td>\n",
       "      <td>18.294191</td>\n",
       "      <td>103.706008</td>\n",
       "      <td>35160.720197</td>\n",
       "      <td>USD</td>\n",
       "      <td>205026</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202</td>\n",
       "      <td>18.294191</td>\n",
       "      <td>103.714991</td>\n",
       "      <td>16834.905270</td>\n",
       "      <td>USD</td>\n",
       "      <td>205027</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202</td>\n",
       "      <td>18.303174</td>\n",
       "      <td>103.661092</td>\n",
       "      <td>95840.912659</td>\n",
       "      <td>USD</td>\n",
       "      <td>205028</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202</td>\n",
       "      <td>18.303174</td>\n",
       "      <td>103.670075</td>\n",
       "      <td>10773.933441</td>\n",
       "      <td>USD</td>\n",
       "      <td>205029</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>202</td>\n",
       "      <td>18.303174</td>\n",
       "      <td>103.714991</td>\n",
       "      <td>30829.250486</td>\n",
       "      <td>USD</td>\n",
       "      <td>205030</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453243</th>\n",
       "      <td>202</td>\n",
       "      <td>17.377909</td>\n",
       "      <td>103.894654</td>\n",
       "      <td>61241.697464</td>\n",
       "      <td>USD</td>\n",
       "      <td>658269</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453244</th>\n",
       "      <td>202</td>\n",
       "      <td>17.377909</td>\n",
       "      <td>103.903637</td>\n",
       "      <td>102724.971329</td>\n",
       "      <td>USD</td>\n",
       "      <td>658270</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453245</th>\n",
       "      <td>202</td>\n",
       "      <td>17.386892</td>\n",
       "      <td>103.876688</td>\n",
       "      <td>97660.766819</td>\n",
       "      <td>USD</td>\n",
       "      <td>658271</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453246</th>\n",
       "      <td>202</td>\n",
       "      <td>17.386892</td>\n",
       "      <td>103.885671</td>\n",
       "      <td>33040.200964</td>\n",
       "      <td>USD</td>\n",
       "      <td>658272</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453247</th>\n",
       "      <td>202</td>\n",
       "      <td>17.386892</td>\n",
       "      <td>103.894654</td>\n",
       "      <td>157707.479144</td>\n",
       "      <td>USD</td>\n",
       "      <td>658273</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>453248 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        category_id   latitude   longitude          value value_unit  \\\n",
       "0               202  18.294191  103.706008   35160.720197        USD   \n",
       "1               202  18.294191  103.714991   16834.905270        USD   \n",
       "2               202  18.303174  103.661092   95840.912659        USD   \n",
       "3               202  18.303174  103.670075   10773.933441        USD   \n",
       "4               202  18.303174  103.714991   30829.250486        USD   \n",
       "...             ...        ...         ...            ...        ...   \n",
       "453243          202  17.377909  103.894654   61241.697464        USD   \n",
       "453244          202  17.377909  103.903637  102724.971329        USD   \n",
       "453245          202  17.386892  103.876688   97660.766819        USD   \n",
       "453246          202  17.386892  103.885671   33040.200964        USD   \n",
       "453247          202  17.386892  103.894654  157707.479144        USD   \n",
       "\n",
       "        deductible  cover  impf_  region_id  centr_D  \n",
       "0           205026      0    202        NaN      599  \n",
       "1           205027      0    202        NaN      599  \n",
       "2           205028      0    202        NaN      599  \n",
       "3           205029      0    202        NaN      599  \n",
       "4           205030      0    202        NaN      599  \n",
       "...            ...    ...    ...        ...      ...  \n",
       "453243      658269      0    202        NaN      835  \n",
       "453244      658270      0    202        NaN      835  \n",
       "453245      658271      0    202        NaN      835  \n",
       "453246      658272      0    202        NaN      835  \n",
       "453247      658273      0    202        NaN      835  \n",
       "\n",
       "[453248 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_future.exposures.gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXP bbox (lat_min,lat_max,lon_min,lon_max): (5.762692553363922, 20.45841426613888, 97.4537335977091, 105.6284026831805)\n",
      "HAZ bbox (lat_min,lat_max,lon_min,lon_max): (5.130434782609598, 21.0, 96.0, 105.99999999999943)\n",
      "{'n_exp': 453248, 'min_km': 0.03087581878708878, 'median_km': 9.445195984526364, 'p95_km': 14.206454004224458, 'max_km': 16.660523729508746, 'within_km': {1: 2638, 5: 64472, 10: 254083, 25: 453248, 50: 453248}}\n",
      "{'centr_D_nan_frac': 0.0, 'centr_D_unique': 975}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "exp_lat = entity_future.exposures.gdf[\"latitude\"].to_numpy(dtype=float)\n",
    "exp_lon = entity_future.exposures.gdf[\"longitude\"].to_numpy(dtype=float)\n",
    "haz_lat = hazard_future.centroids.coord[:, 0].astype(float)\n",
    "haz_lon = hazard_future.centroids.coord[:, 1].astype(float)\n",
    "\n",
    "# ---- Quick bbox reject test ----\n",
    "exp_bbox = (exp_lat.min(), exp_lat.max(), exp_lon.min(), exp_lon.max())\n",
    "haz_bbox = (haz_lat.min(), haz_lat.max(), haz_lon.min(), haz_lon.max())\n",
    "print(\"EXP bbox (lat_min,lat_max,lon_min,lon_max):\", exp_bbox)\n",
    "print(\"HAZ bbox (lat_min,lat_max,lon_min,lon_max):\", haz_bbox)\n",
    "\n",
    "# ---- Project to local-equirectangular (km) for fast KDTree ----\n",
    "R = 6371.0088  # km\n",
    "phi0 = np.deg2rad(np.mean(exp_lat))  # reference latitude\n",
    "\n",
    "def to_xy_km(lat_deg, lon_deg):\n",
    "    lat = np.deg2rad(lat_deg)\n",
    "    lon = np.deg2rad(lon_deg)\n",
    "    x = R * lon * np.cos(phi0)\n",
    "    y = R * lat\n",
    "    return np.column_stack([x, y])\n",
    "\n",
    "haz_xy = to_xy_km(haz_lat, haz_lon)\n",
    "exp_xy = to_xy_km(exp_lat, exp_lon)\n",
    "\n",
    "# ---- KDTree NN distances from exposures to nearest hazard centroid ----\n",
    "tree = cKDTree(haz_xy)\n",
    "dist_km, _ = tree.query(exp_xy, k=1, workers=-1)  # vectorized; returns km due to our projection\n",
    "\n",
    "# ---- Stats + thresholds ----\n",
    "thr_km = [1, 5, 10, 25, 50]\n",
    "stats = {\n",
    "    \"n_exp\": exp_xy.shape[0],\n",
    "    \"min_km\": float(np.min(dist_km)),\n",
    "    \"median_km\": float(np.median(dist_km)),\n",
    "    \"p95_km\": float(np.quantile(dist_km, 0.95)),\n",
    "    \"max_km\": float(np.max(dist_km)),\n",
    "    \"within_km\": {t: int((dist_km <= t).sum()) for t in thr_km},\n",
    "}\n",
    "print(stats)\n",
    "\n",
    "# ---- If many distances are huge, assignments likely miss; check your centr_D mapping ----\n",
    "if \"centr_D\" in entity_future.exposures.gdf.columns:\n",
    "    centr = entity_future.exposures.gdf[\"centr_D\"].to_numpy()\n",
    "    print({\n",
    "        \"centr_D_nan_frac\": float(np.mean(np.isnan(centr))),\n",
    "        \"centr_D_unique\": int(len(np.unique(centr[~np.isnan(centr)]))),\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_asset_centroids': 975, 'nonzero_on_assets': 0, 'min': -3.5, 'p50': -1.6465805768966675, 'p95': -1.3205202341079714, 'max': -1.1166930198669434}\n",
      "{'near_zero_count': 975, 'near_zero_frac': 1.0}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m sub_all \u001b[38;5;241m=\u001b[39m hazard_future\u001b[38;5;241m.\u001b[39mintensity[:, uniq_idx]      \u001b[38;5;66;03m# sparse (events x asset-centroids)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m max_per_centroid \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(sub_all\u001b[38;5;241m.\u001b[39mmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m({\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masset_centroids_with_any_signal\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m((\u001b[43mmax_per_centroid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m)\u001b[38;5;241m.\u001b[39msum()),\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masset_centroids_all_zero\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m((max_per_centroid \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m thr)\u001b[38;5;241m.\u001b[39msum()),\n\u001b[0;32m     34\u001b[0m })\n",
      "File \u001b[1;32mc:\\Users\\gkalomalos\\Projects\\unu\\ERA_projects_SWORD\\climada_env\\lib\\site-packages\\scipy\\sparse\\_base.py:337\u001b[0m, in \u001b[0;36mspmatrix.__bool__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnnz \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe truth value of an array with more than one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    338\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melement is ambiguous. Use a.any() or a.all().\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# exposures → unique centroid indices used by assets\n",
    "exp_idx = entity_future.exposures.gdf[\"centr_D\"].astype(int).to_numpy()\n",
    "uniq_idx = np.unique(exp_idx)\n",
    "\n",
    "# pick one event row to test (e.g., event 0)\n",
    "row = hazard_future.intensity.getrow(0)             # CSR, shape (1, n_centroids)\n",
    "vals = row[:, uniq_idx].toarray().ravel()           # intensities at asset-used centroids\n",
    "\n",
    "print({\n",
    "    \"n_asset_centroids\": uniq_idx.size,\n",
    "    \"nonzero_on_assets\": int((vals > 0).sum()),\n",
    "    \"min\": float(vals.min()) if vals.size else None,\n",
    "    \"p50\": float(np.median(vals)) if vals.size else None,\n",
    "    \"p95\": float(np.quantile(vals, 0.95)) if vals.size else None,\n",
    "    \"max\": float(vals.max()) if vals.size else None,\n",
    "})\n",
    "\n",
    "# optional: treat “near zero” as below a domain threshold\n",
    "thr = 1e-6\n",
    "print({\n",
    "    \"near_zero_count\": int((vals <= thr).sum()),\n",
    "    \"near_zero_frac\": float((vals <= thr).mean()),\n",
    "})\n",
    "\n",
    "# overall check across ALL events on asset-used centroids\n",
    "sub_all = hazard_future.intensity[:, uniq_idx]      # sparse (events x asset-centroids)\n",
    "max_per_centroid = np.asarray(sub_all.max(axis=0)).ravel()\n",
    "print({\n",
    "    \"asset_centroids_with_any_signal\": int((max_per_centroid > 0).sum()),\n",
    "    \"asset_centroids_all_zero\": int((max_per_centroid <= thr).sum()),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 21.        ,  96.        ],\n",
       "       [ 21.        ,  96.2173913 ],\n",
       "       [ 21.        ,  96.43478261],\n",
       "       ...,\n",
       "       [  5.13043478, 105.56521739],\n",
       "       [  5.13043478, 105.7826087 ],\n",
       "       [  5.13043478, 106.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hazard_future.centroids.coord\n",
    "\n",
    "# array([[ 21.        ,  96.        ],\n",
    "#        [ 21.        ,  96.2173913 ],\n",
    "#        [ 21.        ,  96.43478261],\n",
    "#        ...,\n",
    "#        [  5.13043478, 105.56521739],\n",
    "#        [  5.13043478, 105.7826087 ],\n",
    "#        [  5.13043478, 106.        ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x3478 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 17390 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hazard_future.intensity\n",
    "\n",
    "# <5x3478 sparse matrix of type '<class 'numpy.float32'>'\n",
    "# \twith 17390 stored elements in Compressed Sparse Row format>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example CLIMADA\n",
    "entity_present = Entity.from_excel(\n",
    "    DATA_ENTITIES_DIR / \"entity_TODAY_THA_D_tree_crops.xlsx\"\n",
    ")\n",
    "# Set exposure ref year\n",
    "entity_present.exposures.ref_year = 2024\n",
    "entity_present.check()\n",
    "\n",
    "# Set future Entity\n",
    "entity_future = deepcopy(entity_present)\n",
    "entity_future.exposures.ref_year = 2050\n",
    "growth = 0.029\n",
    "entity_future.exposures.gdf[\"value\"] = entity_future.exposures.gdf.value.values * (1 + growth) ** (\n",
    "    entity_future.exposures.ref_year - entity_present.exposures.ref_year\n",
    ")\n",
    "entity_future.check()\n",
    "\n",
    "# Set present Hazard\n",
    "hazard_present = Hazard.from_hdf5(\n",
    "    DATA_HAZARDS_DIR / \"hazard_D_THA_historical.h5\",\n",
    ")\n",
    "hazard_present.units = \"m\"\n",
    "hazard_present.centroids.set_geometry_points()\n",
    "hazard_present.intensity_thres = -4\n",
    "hazard_present.check()\n",
    "\n",
    "# Set future hazard\n",
    "hazard_future = Hazard.from_hdf5(\n",
    "    DATA_HAZARDS_DIR / \"hazard_HW_EGY_rcp45.h5\",\n",
    "    attrs={\n",
    "        \"frequency\": np.array([0.5, 0.2, 0.1, 0.04]),\n",
    "        \"event_id\": np.array([1, 2, 3, 4]),\n",
    "        \"units\": \"number of days\",\n",
    "    },\n",
    "    haz_type=\"HW\",\n",
    "    band=[1, 2, 3, 4],\n",
    ")\n",
    "hazard_future.units = \"number of days\"\n",
    "hazard_future.centroids.set_geometry_points()\n",
    "hazard_future.intensity_thres = 0\n",
    "hazard_future.check()\n",
    "\n",
    "# Calculate impact\n",
    "impact_present = ImpactCalc(\n",
    "    entity_present.exposures, entity_present.impact_funcs, hazard_present\n",
    ").impact()\n",
    "impact_future = ImpactCalc(\n",
    "    entity_future.exposures, entity_future.impact_funcs, hazard_future\n",
    ").impact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List DataTypeInfos\n",
    "data_type_infos = client.list_data_type_infos()\n",
    "exposure_data_types = [\n",
    "    data_type_info.data_type\n",
    "    for data_type_info in data_type_infos\n",
    "    if data_type_info.data_type_group == \"exposures\"\n",
    "]\n",
    "hazard_data_types = [\n",
    "    data_type_info.data_type\n",
    "    for data_type_info in data_type_infos\n",
    "    if data_type_info.data_type_group == \"hazard\"\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Available Exposure data types in CLIMADA API for all countries:\\n{exposure_data_types}\")\n",
    "print(\n",
    "    f\"Available Hazard data types in CLIMADA API for all countries:\\n{hazard_data_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available country Exposures and Hazard data types\n",
    "dataset_infos = client.list_dataset_infos(\n",
    "    properties={\n",
    "        \"country_name\": country_name,\n",
    "    }\n",
    ")\n",
    "\n",
    "exposure_data_types = list(\n",
    "    set(\n",
    "        [\n",
    "            dataset_info.data_type.data_type\n",
    "            for dataset_info in dataset_infos\n",
    "            if dataset_info.data_type.data_type_group == \"exposures\"\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "hazard_data_types = list(\n",
    "    set(\n",
    "        [\n",
    "            dataset_info.data_type.data_type\n",
    "            for dataset_info in dataset_infos\n",
    "            if dataset_info.data_type.data_type_group == \"hazard\"\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Available Exposure data types in CLIMADA API for {country_name}: {exposure_data_types}\")\n",
    "print(\n",
    "    f\"Available Hazard data types in CLIMADA API for {country_name}: {hazard_data_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available climate scenarios and time horizons for specific hazard type in countries Thailand and Egypt\n",
    "if hazard_type == \"river_flood\" or \"wildfire\":\n",
    "    available_scenarios = list(\n",
    "        set(\n",
    "            [\n",
    "                dataset_info.properties[\"climate_scenario\"]\n",
    "                for dataset_info in dataset_infos\n",
    "                if dataset_info.data_type.data_type == hazard_type\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    available_time_horizons = list(\n",
    "        set(\n",
    "            [\n",
    "                dataset_info.properties[\"year_range\"]\n",
    "                for dataset_info in dataset_infos\n",
    "                if dataset_info.data_type.data_type == hazard_type\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "if hazard_type == \"tropical_cyclone\":\n",
    "    available_scenarios = list(\n",
    "        set(\n",
    "            [\n",
    "                dataset_info.properties[\"climate_scenario\"]\n",
    "                for dataset_info in dataset_infos\n",
    "                if dataset_info.data_type.data_type == hazard_type\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    available_time_horizons = list(\n",
    "        set(\n",
    "            [\n",
    "                dataset_info.properties.get(\"ref_year\")\n",
    "                for dataset_info in dataset_infos\n",
    "                if dataset_info.data_type.data_type == hazard_type\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "if hazard_type == \"earthquake\":\n",
    "    available_scenarios = []\n",
    "    available_time_horizons = []\n",
    "if hazard_type == \"flood\":\n",
    "    available_scenarios = []\n",
    "    available_time_horizons = list(\n",
    "        set(\n",
    "            [\n",
    "                dataset_info.properties[\"year_range\"]\n",
    "                for dataset_info in dataset_infos\n",
    "                if dataset_info.data_type.data_type == hazard_type\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"Available climate scenarios for hazard type {hazard_type} in country {country_name}: {available_scenarios}\"\n",
    ")\n",
    "print(\n",
    "    f\"Available time horizons for hazard type {hazard_type} in country {country_name}: {available_time_horizons}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available country Hazard DatasetInfos\n",
    "hazard_dataset_infos = client.list_dataset_infos(\n",
    "    properties={\n",
    "        \"data_type\": \"river_flood\",\n",
    "        \"country_name\": \"Thailand\",\n",
    "        \"climate_scenario\": \"rcp26\",\n",
    "        \"year_range\": \"2030_2050\",\n",
    "    }\n",
    ")\n",
    "hazard_dataset_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "class DataFrameSQLite:\n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "\n",
    "    def save_dataframe(self, df, table_name):\n",
    "        \"\"\"\n",
    "        Save a DataFrame to the SQLite database.\n",
    "        \n",
    "        :param df: DataFrame to save.\n",
    "        :param table_name: Name of the table to save the DataFrame to.\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "            print(f\"DataFrame saved to {table_name} table.\")\n",
    "\n",
    "    def read_dataframe(self, table_name):\n",
    "        \"\"\"\n",
    "        Read a DataFrame from the SQLite database.\n",
    "        \n",
    "        :param table_name: Name of the table to read the DataFrame from.\n",
    "        :return: DataFrame read from the database.\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            df = pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "        return df\n",
    "\n",
    "# Example usage\n",
    "db_path = 'my_data.db'  # Path to your SQLite database file\n",
    "df_sqlite = DataFrameSQLite(db_path)\n",
    "\n",
    "# Assuming you have a DataFrame `df` to save\n",
    "df_sqlite.save_dataframe(exp_gdf, 'exposure')\n",
    "\n",
    "# To read the saved DataFrame from the database\n",
    "df_read = df_sqlite.read_dataframe('exposure')\n",
    "print(df_read)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "class ExcelToSQLite:\n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "\n",
    "    def read_excel_to_df(self, excel_path, sheet_name):\n",
    "        \"\"\"Reads a specified sheet from an Excel file into a DataFrame.\"\"\"\n",
    "        return pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "\n",
    "    def adjust_df_columns(self, df, columns_to_keep=None):\n",
    "        \"\"\"Adjusts DataFrame columns based on the provided list. If None, keeps all columns.\"\"\"\n",
    "        if columns_to_keep is not None:\n",
    "            df = df[columns_to_keep]\n",
    "        return df\n",
    "\n",
    "    def save_df_to_sqlite(self, df, table_name):\n",
    "        \"\"\"Saves a DataFrame to an SQLite table, appending data if the table already exists.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            df.to_sql(table_name, conn, if_exists=\"append\", index=False)\n",
    "\n",
    "    def read_table(self, table_name):\n",
    "        \"\"\"Reads a table from SQLite database into a DataFrame.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            return pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "db_path = \"climadera.db\"  # Path to your SQLite database file\n",
    "excel_path = DATA_ENTITIES_DIR / \"3_entity_TODAY_THAI_D_USD_1_modified.xlsx\"  # Path to your Excel file\n",
    "excel_to_sqlite = ExcelToSQLite(db_path)\n",
    "\n",
    "# Define the sheets and corresponding table names\n",
    "sheets_tables = {\n",
    "    \"assets\": \"exposures\",\n",
    "    \"impact_functions\": \"impact_functions\",\n",
    "    \"measures\": \"measures\",\n",
    "    \"discount\": \"discount_rates\",\n",
    "    \"names\": \"names\",\n",
    "}\n",
    "\n",
    "# Iterate over sheets and tables, read, adjust (if needed), and save to SQLite\n",
    "for sheet, table in sheets_tables.items():\n",
    "    df = excel_to_sqlite.read_excel_to_df(excel_path, sheet)\n",
    "\n",
    "    # Here you can define which columns to keep for each table if needed, e.g.:\n",
    "    # if table == 'exposures':\n",
    "    #     columns_to_keep = ['Column1', 'Column2']\n",
    "    #     df = excel_to_sqlite.adjust_df_columns(df, columns_to_keep)\n",
    "    # else:\n",
    "    #     df = excel_to_sqlite.adjust_df_columns(df)\n",
    "\n",
    "    excel_to_sqlite.save_df_to_sqlite(df, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "class ExcelToSQLite:\n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "\n",
    "    def read_excel_and_save(self, excel_path, tabs_columns_mapping):\n",
    "        \"\"\"\n",
    "        Read specified tabs from an Excel file and save them to SQLite database.\n",
    "\n",
    "        :param excel_path: Path to the Excel file.\n",
    "        :param tabs_columns_mapping: Dict mapping tab names to their columns adjustments.\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            for tab, columns in tabs_columns_mapping.items():\n",
    "                df = pd.read_excel(excel_path, sheet_name=tab)\n",
    "\n",
    "                # Adjust columns if specified\n",
    "                if columns:\n",
    "                    df = df[columns]\n",
    "\n",
    "                df.to_sql(tab, conn, if_exists=\"append\", index=False)\n",
    "                print(f\"Data from {tab} tab saved to {tab} table.\")\n",
    "\n",
    "    def read_table(self, table_name):\n",
    "        \"\"\"\n",
    "        Read data from a specified table in the SQLite database.\n",
    "\n",
    "        :param table_name: Name of the table to read data from.\n",
    "        :return: DataFrame with the table data.\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            return pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "db_path = \"climadera.db\"  # Path to your SQLite database file\n",
    "excel_path = DATA_ENTITIES_DIR / \"3_entity_TODAY_THAI_D_USD_1_modified.xlsx\"  # Path to your Excel file\n",
    "tabs_columns_mapping = {\n",
    "    \"assets\": None,  # Specify columns as a list if you want to adjust them, or None to include all\n",
    "    \"impact_functions\": None,\n",
    "    \"measures\": None,\n",
    "    \"discount\": None,\n",
    "    \"names\": None,\n",
    "}\n",
    "\n",
    "excel_to_sqlite = ExcelToSQLite(db_path)\n",
    "excel_to_sqlite.read_excel_and_save(excel_path, tabs_columns_mapping)\n",
    "\n",
    "# Reading data from one of the tables\n",
    "df_assets = excel_to_sqlite.read_table(\"assets\")\n",
    "print(df_assets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climada_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
